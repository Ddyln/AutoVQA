{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead80586",
   "metadata": {},
   "source": [
    "# AutoVQA - Getting Started Guide\n",
    "\n",
    "A Python library for automated visual question answering (VQA) data generation and processing.\n",
    "\n",
    "This notebook covers the following modules:\n",
    "1. **Collect** - Download and prepare VQA datasets\n",
    "2. **Preprocess** - Image preprocessing utilities\n",
    "3. **Augment** - Generate VQA data using LLMs\n",
    "4. **EDA** - Exploratory Data Analysis\n",
    "5. **Filter** - Data filtering and cleaning\n",
    "6. **Balance** - Dataset balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8f7ed",
   "metadata": {},
   "source": [
    "<!-- ## Installation\n",
    "\n",
    "### Install from PyPI\n",
    "```bash\n",
    "pip install autovqa\n",
    "```\n",
    "\n",
    "### Install from source (for development)\n",
    "```bash\n",
    "git clone https://github.com/Ddyln/AutoVQA.git\n",
    "cd AutoVQA\n",
    "pip install -e \".[dev]\"\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eafefc3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Collect Module\n",
    "\n",
    "The collect module provides utilities to download VQA datasets and images.\n",
    "\n",
    "### Entry Functions\n",
    "- `download_default_data()` - This function downloads the default VQA dataset including text data and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f33949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovqa.collect import download_default_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the default dataset to a specified directory\n",
    "# This will:\n",
    "# 1. Download text data zip\n",
    "# 2. Extract the zip file\n",
    "# 3. Download images from URLs in the extracted JSON\n",
    "download_default_data(output=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf45108",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Preprocess Module\n",
    "\n",
    "The preprocess module provides image preprocessing utilities for VQA tasks.\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "1. Resize - Resize while maintaining aspect ratio\n",
    "2. Pad - Pad to exact target size\n",
    "3. Denoise - Reduce image noise\n",
    "4. Color Correction - Improve contrast using CLAHE\n",
    "5. Sharpening - Apply unsharp mask\n",
    "6. Normalize (optional) - Normalize pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovqa.preprocess.main import preprocess_image, run_pipeline\n",
    "\n",
    "print(\"Preprocess module imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0f664",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Single Image\n",
    "\n",
    "Process a single image through the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52aeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Preprocess a single image\n",
    "processed_image = preprocess_image(\n",
    "    image_path=\"path/to/image.jpg\",\n",
    "    target_size=(480, 640),  # (height, width)\n",
    "    do_normalize=False\n",
    ")\n",
    "\n",
    "# Save the processed image\n",
    "cv2.imwrite(\"processed_image.jpg\", processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94d3ad",
   "metadata": {},
   "source": [
    "### 2.2 Batch Preprocess Images\n",
    "\n",
    "Process all images in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing pipeline on all images in a folder\n",
    "run_pipeline(\n",
    "    input_folder=\"./raw_images\",\n",
    "    output_folder=\"./processed_images\",\n",
    "    do_normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea3832e",
   "metadata": {},
   "source": [
    "### 2.3 Individual Preprocessing Functions\n",
    "\n",
    "You can also use individual preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5968a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovqa.preprocess.image.resize import resize_image, pad_image\n",
    "from autovqa.preprocess.image.denoise import denoise_image\n",
    "from autovqa.preprocess.image.color_correction import color_correction\n",
    "from autovqa.preprocess.image.sharpening import unsharp_mask\n",
    "from autovqa.preprocess.image.normalize import normalize_image\n",
    "\n",
    "# Example: Manual preprocessing pipeline\n",
    "image = cv2.imread(\"image.jpg\")\n",
    "image = resize_image(image, target_size=(480, 640))\n",
    "image = pad_image(image, target_size=(480, 640))\n",
    "image = denoise_image(image)\n",
    "image = color_correction(image)\n",
    "image = unsharp_mask(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75d294",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Augment Module\n",
    "\n",
    "The augment module generates VQA question-answer pairs using LLMs (e.g., Gemini).\n",
    "\n",
    "### Configuration\n",
    "Before using the augment module, you need to set up a config file at:\n",
    "- Linux: `~/.config/autovqa/config.toml`\n",
    "- Windows: `C:\\Users\\<user>\\AppData\\Local\\autovqa\\autovqa\\config.toml`\n",
    "\n",
    "Example configuration is located at `./src/autovqa/augment/sample_config.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9557f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovqa.augment.client import AugmentClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cee4b5",
   "metadata": {},
   "source": [
    "### 3.1 Generate VQA Data from Images\n",
    "\n",
    "Generate question-answer pairs for images using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the augment client\n",
    "client = AugmentClient(service_name=\"gemini\")\n",
    "\n",
    "# Generate VQA data for all images in a folder\n",
    "results = client.run_pipeline(\n",
    "    image_folder_dir=\"./images\",\n",
    "    output_json_path=\"./output/augmented_vqa.json\"\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(results)} QA pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c722cf",
   "metadata": {},
   "source": [
    "### 3.2 Generate for Single Image\n",
    "\n",
    "Generate VQA data for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate for a single image\n",
    "client = AugmentClient(service_name=\"gemini\")\n",
    "response = client.generate_response(\"path/to/image.jpg\")\n",
    "\n",
    "if response:\n",
    "    formatted = client.format_response(\n",
    "        json_response=response.model_dump(),\n",
    "        image_path=\"path/to/image.jpg\"\n",
    "    )\n",
    "    print(formatted)\n",
    "else:\n",
    "\tprint(\"No response generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29795ccd",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. EDA Module (Exploratory Data Analysis)\n",
    "\n",
    "The EDA module performs comprehensive analysis on VQA data:\n",
    "- Data cleaning and deduplication\n",
    "- Feature extraction (scene types, main objects)\n",
    "- Statistical analysis\n",
    "- Report generation (Excel files)\n",
    "\n",
    "### Parameters\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| data | list | required | JSON data to analyze |\n",
    "| output_dir | str | \"report\" | Directory for reports |\n",
    "| generate_report | bool | True | Generate Excel reports |\n",
    "| aggregation_type | str | \"median\" | Score aggregation (median/mean/max/min) |\n",
    "| history | list | None | Track operations history |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb204d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from autovqa import eda_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96874cf1",
   "metadata": {},
   "source": [
    "### 4.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your JSON data\n",
    "DATA_PATH = \"../data/data_text/Data/combined_dataset/datasetQA_combined.json\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(DATA_PATH):\n",
    "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} records\")\n",
    "    print(f\"Sample record keys: {list(data[0].keys())}\")\n",
    "else:\n",
    "    print(f\"Data file not found at {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH to point to your data file\")\n",
    "    data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157f666",
   "metadata": {},
   "source": [
    "### 4.2 Run EDA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EDA Pipeline\n",
    "if data:\n",
    "    df = eda_pipeline(\n",
    "        data=data,\n",
    "        output_dir=\"./reports\",\n",
    "        generate_report=True,\n",
    "        aggregation_type=\"median\"\n",
    "    )\n",
    "    \n",
    "    print(f\"EDA completed\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a45a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the processed DataFrame\n",
    "if 'df' in dir():\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fde4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DataFrame statistics\n",
    "if 'df' in dir():\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913678e",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Filter Module\n",
    "\n",
    "The filter module filters data based on quality labels and thresholds.\n",
    "\n",
    "### Parameters\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| data | DataFrame | required | DataFrame to filter |\n",
    "| column_names | list | None | Columns to check (auto-detects \"Label\" columns) |\n",
    "| threshold | float | 0.5 | Minimum ratio of passed labels (0.0-1.0) |\n",
    "| keep_columns | list | None | Columns to keep in result |\n",
    "| show_stats | bool | True | Show filtering statistics |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e891d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovqa import filter_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072cb4c",
   "metadata": {},
   "source": [
    "### 5.1 Run Filter Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceca89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Filter Pipeline\n",
    "if 'df' in dir():\n",
    "    initial_count = len(df)\n",
    "    print(f\"Initial records: {initial_count}\")\n",
    "    \n",
    "    df_filtered = filter_pipeline(\n",
    "        data=df,\n",
    "        threshold=0.5,  # Keep records where >= 50% of labels passed\n",
    "        show_stats=True\n",
    "    )\n",
    "    \n",
    "    filtered_count = len(df_filtered)\n",
    "    print(f\"Records after filtering: {filtered_count}\")\n",
    "    print(f\"Removed: {initial_count - filtered_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9543111",
   "metadata": {},
   "source": [
    "### 5.2 Filter with Custom Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170beff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Stricter filtering with higher threshold\n",
    "df_strict = filter_pipeline(\n",
    "    data=df,\n",
    "    threshold=0.7,  # Stricter: >= 70% of labels must pass\n",
    "    show_stats=True\n",
    ")\n",
    "\n",
    "# Example: Lenient filtering with lower threshold\n",
    "df_lenient = filter_pipeline(\n",
    "    data=df,\n",
    "    threshold=0.3,  # Lenient: >= 30% of labels must pass\n",
    "    show_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94040d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Balance Module\n",
    "\n",
    "The balance module ensures balanced class distributions in your dataset.\n",
    "\n",
    "### Parameters\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| df_raw | DataFrame | required | Input DataFrame |\n",
    "| numeric_columns | list | config | Columns for score computation |\n",
    "| feature_columns | list | config | Columns to balance |\n",
    "| reason_depth_weight | int | 4 | Weight for reason depth |\n",
    "| percent_min_samples | float | 0.01 | Min percent to keep labels |\n",
    "| top_percent | float | 0.9 | Keep top X% labels by frequency |\n",
    "| limit_percent | float | 10 | Max percent difference between classes |\n",
    "| keep_outliers | bool | True | Keep rare labels |\n",
    "| output_path | str | None | CSV save path |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovqa import balancer_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0d64f",
   "metadata": {},
   "source": [
    "### 6.1 Run Balancer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cddf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Balancer Pipeline\n",
    "if 'df_filtered' in dir():\n",
    "    pre_balance_count = len(df_filtered)\n",
    "    print(f\"Records before balancing: {pre_balance_count}\")\n",
    "    \n",
    "    df_balanced = balancer_pipeline(\n",
    "        df_raw=df_filtered,\n",
    "        output_path=None  # Set a path to save as CSV\n",
    "    )\n",
    "    \n",
    "    balanced_count = len(df_balanced)\n",
    "    print(f\"Records after balancing: {balanced_count}\")\n",
    "    print(f\"Removed: {pre_balance_count - balanced_count} records\")\n",
    "else:\n",
    "    print(\"Filtered DataFrame not found. Please run the filter pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337679c1",
   "metadata": {},
   "source": [
    "### 6.2 Balance with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom balancing parameters\n",
    "df_balanced = balancer_pipeline(\n",
    "    df_raw=df_filtered,\n",
    "    percent_min_samples=0.02,  # Keep labels with >= 2% samples\n",
    "    top_percent=0.85,  # Keep top 85% labels by frequency\n",
    "    limit_percent=15,  # Max 15% difference between classes\n",
    "    keep_outliers=False,  # Remove rare labels\n",
    "    output_path=\"./output/balanced_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce3584",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Pipeline Example\n",
    "\n",
    "Here is a complete end-to-end workflow combining all processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vqa_data(\n",
    "    data_path: str,\n",
    "    output_dir: str = \"./output\",\n",
    "    filter_threshold: float = 0.5,\n",
    "    generate_reports: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete VQA data processing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to JSON data file\n",
    "        output_dir: Directory for outputs\n",
    "        filter_threshold: Threshold for filtering (0.0-1.0)\n",
    "        generate_reports: Whether to generate EDA reports\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed and balanced DataFrame\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    from autovqa import eda_pipeline, filter_pipeline, balancer_pipeline\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"Step 1: Loading data...\")\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"  Loaded {len(data)} records\")\n",
    "    \n",
    "    # Step 2: EDA Pipeline\n",
    "    print(\"Step 2: Running EDA pipeline...\")\n",
    "    df = eda_pipeline(\n",
    "        data=data,\n",
    "        output_dir=os.path.join(output_dir, \"reports\"),\n",
    "        generate_report=generate_reports\n",
    "    )\n",
    "    print(f\"  EDA complete: {len(df)} records\")\n",
    "    \n",
    "    # Step 3: Filter Pipeline\n",
    "    print(\"Step 3: Running filter pipeline...\")\n",
    "    df = filter_pipeline(\n",
    "        data=df,\n",
    "        threshold=filter_threshold,\n",
    "        show_stats=False\n",
    "    )\n",
    "    print(f\"  Filtering complete: {len(df)} records\")\n",
    "    \n",
    "    # Step 4: Balancer Pipeline\n",
    "    print(\"Step 4: Running balancer pipeline...\")\n",
    "    df = balancer_pipeline(\n",
    "        df_raw=df,\n",
    "        output_path=os.path.join(output_dir, \"balanced_data.csv\")\n",
    "    )\n",
    "    print(f\"  Balancing complete: {len(df)} records\")\n",
    "    \n",
    "    print(\"Pipeline complete!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "df_final = process_vqa_data(\n",
    "    data_path=\"path/to/your/data.json\",\n",
    "    output_dir=\"./output\",\n",
    "    filter_threshold=0.5,\n",
    "    generate_reports=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f33c0",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving Results\n",
    "\n",
    "Save your processed data in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df_balanced.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# Save as JSON\n",
    "df_balanced.to_json(\"output.json\", orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "# Save as Parquet\n",
    "df_balanced.to_parquet(\"output.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_vivqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
